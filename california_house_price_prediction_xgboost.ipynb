{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48543c61",
      "metadata": {
        "id": "48543c61"
      },
      "source": [
        "### Import Libraries\n",
        "This cell imports essential libraries for data manipulation, numerical operations, and plotting: `numpy`, `pandas`, `matplotlib.pyplot`, and `seaborn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7Ls-TABbagA",
      "metadata": {
        "id": "e7Ls-TABbagA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0045321",
      "metadata": {
        "id": "d0045321"
      },
      "source": [
        "### Load Dataset\n",
        "This cell loads the **original 1990 California Housing dataset** from **OpenML** using `fetch_openml`.  \n",
        "Unlike the simplified version available in `sklearn.datasets`, this dataset includes the categorical\n",
        "feature `ocean_proximity` and reflects the original census data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xbKr-yMCmwyQ",
      "metadata": {
        "id": "xbKr-yMCmwyQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "dataset = fetch_openml(name=\"california_housing\", as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EmPcibV9nK28",
      "metadata": {
        "id": "EmPcibV9nK28"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7dcafdf",
      "metadata": {
        "id": "b7dcafdf"
      },
      "source": [
        "### Dataset Description\n",
        "This cell prints the detailed description of the California Housing dataset, providing information about its features, target variable, and source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rQfi5UQzeSU4",
      "metadata": {
        "id": "rQfi5UQzeSU4"
      },
      "outputs": [],
      "source": [
        "dataset.DESCR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e126532f",
      "metadata": {
        "id": "e126532f"
      },
      "source": [
        "### Separate Features and Target\n",
        "This cell separates the dataset into input features (`X`) and the target variable (`y`),  \n",
        "where the target represents **median house value (in USD)** for each district.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bceHAzs1btvE",
      "metadata": {
        "id": "bceHAzs1btvE"
      },
      "outputs": [],
      "source": [
        "X,y = dataset.data,dataset.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37_dkhqSb9Bi",
      "metadata": {
        "id": "37_dkhqSb9Bi"
      },
      "outputs": [],
      "source": [
        "feature_names = dataset.feature_names\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T3-WgopNcFph",
      "metadata": {
        "id": "T3-WgopNcFph"
      },
      "outputs": [],
      "source": [
        "target_name = dataset.target_names\n",
        "target_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3MPn6A-BcVaG",
      "metadata": {
        "id": "3MPn6A-BcVaG"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDyMPAaWcZyD",
      "metadata": {
        "id": "kDyMPAaWcZyD"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C0G8cIodcdgf",
      "metadata": {
        "id": "C0G8cIodcdgf"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDTqUeHNc9k_",
      "metadata": {
        "id": "XDTqUeHNc9k_"
      },
      "outputs": [],
      "source": [
        "X.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcQCfGSUc-fp",
      "metadata": {
        "id": "vcQCfGSUc-fp"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RpaizA0lc_hI",
      "metadata": {
        "id": "RpaizA0lc_hI"
      },
      "outputs": [],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02ea6da",
      "metadata": {
        "id": "a02ea6da"
      },
      "source": [
        "### Split Data into Training and Test Sets\n",
        "This cell splits the dataset into training and test sets using an **80/20 split**.\n",
        "The test set is kept completely unseen during model training and feature engineering\n",
        "to ensure an unbiased evaluation of model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L-wKITB4dAyL",
      "metadata": {
        "id": "L-wKITB4dAyL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d346fe",
      "metadata": {
        "id": "49d346fe"
      },
      "source": [
        "### Visualize Target Variable Distribution\n",
        "This cell generates two histograms with Kernel Density Estimates (KDE) to visualize the distribution of the target variable (`y`) in both the training and test sets. This helps in understanding if the target distribution is consistent across the splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OPHKVheNU146",
      "metadata": {
        "id": "OPHKVheNU146"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "sns.histplot(y_train, kde=True, ax=axes[0])\n",
        "axes[0].set_title('Distribution of Target Variable (Train Set)')\n",
        "axes[0].set_xlabel('Target Value')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "sns.histplot(y_test, kde=True, ax=axes[1])\n",
        "axes[1].set_title('Distribution of Target Variable (Test Set)')\n",
        "axes[1].set_xlabel('Target Value')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2LPpkEoSTXGG",
      "metadata": {
        "id": "2LPpkEoSTXGG"
      },
      "source": [
        "### Concatenate Training Features and Target (EDA Only)\n",
        "This cell concatenates `X_train` and `y_train` **only for exploratory data analysis (EDA)**.\n",
        "This combined DataFrame is **not used for model training or feature engineering fitting**,\n",
        "preventing any form of data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dHqAMENMH7zO",
      "metadata": {
        "id": "dHqAMENMH7zO"
      },
      "outputs": [],
      "source": [
        "df_train_test = pd.concat([X_train,y_train],axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bec4285",
      "metadata": {
        "id": "0bec4285"
      },
      "source": [
        "### Histograms of All Features in Training Set\n",
        "This cell generates histograms for all features in the `df_train_test` DataFrame. This provides a visual overview of the distribution of each variable in the training data, helping to identify skewness, outliers, and potential transformations needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gxbbc5sme6Q-",
      "metadata": {
        "id": "Gxbbc5sme6Q-"
      },
      "outputs": [],
      "source": [
        "df_train_test.hist(figsize = (12,10),bins = 50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671060da",
      "metadata": {
        "id": "671060da"
      },
      "source": [
        "### Scatter Matrix Plot of Selected Numerical Attributes\n",
        "This cell creates a scatter matrix for selected numerical features from the training data.\n",
        "The plot helps visualize pairwise relationships, distributions, and potential non-linear patterns\n",
        "among key housing attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t2IYtxHyDyii",
      "metadata": {
        "id": "t2IYtxHyDyii"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "attributes = ['housing_median_age',\n",
        "              'total_rooms',\n",
        "              'total_bedrooms',\n",
        "              'population',\n",
        "              'households',\n",
        "              'median_income']\n",
        "scatter_matrix(df_train_test[attributes],figsize = (12,14))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effa7997",
      "metadata": {
        "id": "effa7997"
      },
      "source": [
        "### Feature Engineering Class Definition\n",
        "This cell defines a custom `FeatureEngineer` transformer that follows **scikit-learn's fit/transform API**.\n",
        "All feature engineering steps are **fitted only on training data** and later applied to validation/test data,\n",
        "ensuring no data leakage.\n",
        "\n",
        "The transformer performs:\n",
        "- **Missing value imputation** using KNNImputer for `total_bedrooms`\n",
        "- **Feature creation**, including:\n",
        "  - Income categories (`income_cat`)\n",
        "  - Bedrooms per household\n",
        "  - Log-transformed versions of skewed numerical features\n",
        "- **One-hot encoding** of the `ocean_proximity` categorical feature\n",
        "- **Removal of raw columns** once their engineered counterparts are created\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9NATzG2LSkX",
      "metadata": {
        "id": "D9NATzG2LSkX"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        self.feature_names_out = None\n",
        "        self.imputer = KNNImputer(n_neighbors=5) # Initialize KNNImputer\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit the OneHotEncoder on the 'ocean_proximity' column\n",
        "        self.encoder.fit(X[['ocean_proximity']])\n",
        "        original_feature_names_out = self.encoder.get_feature_names_out(['ocean_proximity'])\n",
        "        # Sanitize feature names for XGBoost compatibility\n",
        "        cleaned_feature_names = [\n",
        "            name.replace('<', '').replace('>', '').replace(' ', '_').replace('-', '_')\n",
        "            for name in original_feature_names_out\n",
        "        ]\n",
        "        self.feature_names_out = cleaned_feature_names\n",
        "\n",
        "        # Fit the KNNImputer on the 'total_bedrooms' column\n",
        "        self.imputer.fit(X[['total_bedrooms']])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Impute missing values in 'total_bedrooms' first\n",
        "        X['total_bedrooms'] = self.imputer.transform(X[['total_bedrooms']])\n",
        "\n",
        "        # Income category\n",
        "        X['income_cat'] = pd.cut(\n",
        "            X['median_income'],\n",
        "            bins=[0,1.5,3,4.5,6,np.inf],\n",
        "            labels=[1,2,3,4,5]\n",
        "        ).astype(int)\n",
        "\n",
        "        # Engineered features\n",
        "        X['bedrooms_per_house'] = X['total_bedrooms'] / X['total_rooms']\n",
        "        X['Log_population'] = np.log1p(X['population'])\n",
        "        X['Log_total_rooms'] = np.log1p(X['total_rooms'])\n",
        "        X['Log_total_bedrooms'] = np.log1p(X['total_bedrooms'])\n",
        "\n",
        "        # One-hot encode 'ocean_proximity' using sklearn's OneHotEncoder\n",
        "        ocean_proximity_encoded = self.encoder.transform(X[['ocean_proximity']])\n",
        "        ocean_proximity_df = pd.DataFrame(ocean_proximity_encoded, columns=self.feature_names_out, index=X.index)\n",
        "\n",
        "        # Drop the original 'ocean_proximity' column and concatenate the new encoded columns\n",
        "        X = X.drop('ocean_proximity', axis=1)\n",
        "        X = pd.concat([X, ocean_proximity_df], axis=1)\n",
        "\n",
        "        # Drop raw columns\n",
        "        X.drop(['total_rooms','total_bedrooms','population'], axis=1, inplace=True)\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "otjyxwE8UPtD",
      "metadata": {
        "id": "otjyxwE8UPtD"
      },
      "source": [
        "### Instantiate Feature Engineer\n",
        "This cell initializes the `feature_engineer` object, which is an instance of our custom `FeatureEngineer` class. This object will encapsulate all the defined feature engineering steps, allowing for consistent application of transformations to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4VMZQeACHT6L",
      "metadata": {
        "id": "4VMZQeACHT6L"
      },
      "outputs": [],
      "source": [
        "feature_engineer = FeatureEngineer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c19d095",
      "metadata": {
        "id": "2c19d095"
      },
      "source": [
        "### Apply Feature Engineering to Training Data\n",
        "This cell **fits and applies** the feature engineering pipeline on the training data (`X_train`).\n",
        "All transformation parameters (imputation, encoding, feature creation) are learned exclusively\n",
        "from the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vs_K_1t-wpW1",
      "metadata": {
        "id": "Vs_K_1t-wpW1"
      },
      "outputs": [],
      "source": [
        "X_train_featured = feature_engineer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e37e9dc",
      "metadata": {
        "id": "4e37e9dc"
      },
      "source": [
        "### Create Featured Training Data for Analysis\n",
        "This cell constructs a feature-engineered version of the training data **without refitting**\n",
        "the feature engineering pipeline.  \n",
        "The engineered features are concatenated with the target variable purely for\n",
        "correlation analysis and visualization purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JGrm6SEZx0u1",
      "metadata": {
        "id": "JGrm6SEZx0u1"
      },
      "outputs": [],
      "source": [
        "df_train_test_featured = pd.concat(\n",
        "    [X_train_featured, y_train.reset_index(drop=True)],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d46f4e",
      "metadata": {
        "id": "e1d46f4e"
      },
      "source": [
        "### Visualize Income Category Distribution\n",
        "This cell generates a bar plot to visualize the distribution of the `IncomeCat` feature in the `X_train_featured` DataFrame, showing the counts for each income category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bOBw9w3yxF2T",
      "metadata": {
        "id": "bOBw9w3yxF2T"
      },
      "outputs": [],
      "source": [
        "income_cat_counts = X_train_featured['income_cat'].value_counts().sort_index()\n",
        "income_cat_counts.plot(kind='bar', figsize=(8, 6))\n",
        "plt.title('Distribution of Income Categories')\n",
        "plt.xlabel('Income Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ff2bb4",
      "metadata": {
        "id": "18ff2bb4"
      },
      "source": [
        "### Visualize District Locations (Longitude vs Latitude)\n",
        "This cell creates a scatter plot of `Longitude` against `Latitude` from `df_train_test`, visualizing the geographical distribution of the housing districts in California."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yk_dS6Gg_cVt",
      "metadata": {
        "id": "yk_dS6Gg_cVt"
      },
      "outputs": [],
      "source": [
        "df_train_test.plot(kind = 'scatter',x = 'longitude',y = 'latitude',grid = True,alpha = 0.3)\n",
        "plt.title('Location of districts in California')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f811bef3",
      "metadata": {
        "id": "f811bef3"
      },
      "source": [
        "### Visualize Median House Value and Population by Location\n",
        "This cell generates a scatter plot mapping `Longitude` and `Latitude`, where the size of the markers represents `Population` and the color indicates `MedHouseVal`. This helps to visualize the spatial distribution of house values and population density."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77SLwClU_5yJ",
      "metadata": {
        "id": "77SLwClU_5yJ"
      },
      "outputs": [],
      "source": [
        "df_train_test.plot(kind = 'scatter',x = 'longitude',y = 'latitude',\n",
        "             s= X_train['population']/100,label = 'population',\n",
        "             c = 'median_house_value',cmap = 'jet',colorbar = True,\n",
        "             sharex = False,figsize = (10,6),alpha = 0.8)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f39a4626",
      "metadata": {
        "id": "f39a4626"
      },
      "source": [
        "### Correlation Matrix Heatmap\n",
        "This cell visualizes the Pearson correlation matrix of the engineered training features.\n",
        "It provides insight into linear relationships and multicollinearity among engineered variables,\n",
        "but does not imply causal relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0H_tJmkWBDU6",
      "metadata": {
        "id": "0H_tJmkWBDU6"
      },
      "outputs": [],
      "source": [
        "corr_matrix = X_train_featured.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Training Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a1a47c",
      "metadata": {
        "id": "c3a1a47c"
      },
      "source": [
        "### Correlation of Features with Target (Training Data Only)\n",
        "This cell computes the correlation between engineered features and the target variable\n",
        "using **training data only**.  \n",
        "The results help identify features that exhibit strong linear association with\n",
        "median house value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae29c06",
      "metadata": {
        "id": "dae29c06"
      },
      "outputs": [],
      "source": [
        "corr_matrix_with_target = df_train_test_featured.corr()\n",
        "correlations_with_target = corr_matrix_with_target['median_house_value'].sort_values(ascending=False)\n",
        "plt.figure(figsize=(8, 5))\n",
        "correlations_with_target.drop('median_house_value').plot(kind='bar')\n",
        "plt.title('Correlation of Features with Target(median_house_value)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Correlation Coefficient')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mU0FBoLPFoFh",
      "metadata": {
        "id": "mU0FBoLPFoFh"
      },
      "outputs": [],
      "source": [
        "corr_matrix = df_train_test_featured.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6016541a",
      "metadata": {
        "id": "6016541a"
      },
      "source": [
        "### Distribution Comparison: Population vs LogPopulation\n",
        "This cell displays two histograms side-by-side, comparing the original `Population` distribution from `df_train_test` with its log-transformed version, `LogPopulation`, from `df_train_test_featured`. This illustrates the effect of log transformation on the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lAasrVl4Mjw1",
      "metadata": {
        "id": "lAasrVl4Mjw1"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "df_train_test['population'].hist(ax=axes[0], bins=50)\n",
        "axes[0].set_title('Population Distribution')\n",
        "axes[0].set_xlabel('Population')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "df_train_test_featured['Log_population'].hist(ax=axes[1], bins=50)\n",
        "axes[1].set_title('LogPopulation Distribution')\n",
        "axes[1].set_xlabel('Log(Population)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2edd18",
      "metadata": {
        "id": "3c2edd18"
      },
      "source": [
        "### Distribution Comparison: AveRooms vs LogAveRooms\n",
        "This cell displays two histograms side-by-side, comparing the original `AveRooms` distribution from `df_train_test` with its log-transformed version, `LogAveRooms`, from `df_train_test_featured`. This visualizes how the log transformation affects the distribution of average rooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbYgdnLHNbFp",
      "metadata": {
        "id": "cbYgdnLHNbFp"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "df_train_test['total_rooms'].hist(ax=axes[0], bins=50)\n",
        "axes[0].set_title('Total Rooms Distribution')\n",
        "axes[0].set_xlabel('Total Rooms')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "df_train_test_featured['Log_total_rooms'].hist(ax=axes[1], bins=50)\n",
        "axes[1].set_title('Log Total Rooms Distribution')\n",
        "axes[1].set_xlabel('Log(Total Rooms )')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9f32d1",
      "metadata": {
        "id": "4e9f32d1"
      },
      "source": [
        "### Distribution Comparison: AveBedrms vs LogAveBedrms\n",
        "This cell displays two histograms side-by-side, comparing the original `AveBedrms` distribution from `df_train_test` with its log-transformed version, `LogAveBedrms`, from `df_train_test_featured`. This shows the impact of log transformation on the distribution of average bedrooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DsxpOYdwNrRg",
      "metadata": {
        "id": "DsxpOYdwNrRg"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "df_train_test['total_bedrooms'].hist(ax=axes[0], bins=50)\n",
        "axes[0].set_title('Total Bedrooms Distribution')\n",
        "axes[0].set_xlabel('Total Bedrooms')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "df_train_test_featured['Log_total_bedrooms'].hist(ax=axes[1], bins=50)\n",
        "axes[1].set_title('Log Total Bedrooms Distribution')\n",
        "axes[1].set_xlabel('Log(Total Bedrooms)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m1F5JIYPPUht",
      "metadata": {
        "id": "m1F5JIYPPUht"
      },
      "outputs": [],
      "source": [
        "X_train_featured"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b2e43d3",
      "metadata": {
        "id": "6b2e43d3"
      },
      "source": [
        "### Apply Feature Engineering to Test Data\n",
        "This cell applies the `FeatureEngineer` transformation to the `X_test` DataFrame, creating the same new engineered features and dropping specified original columns as done for the training data. The result is stored in `X_test_featured`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7_XKU1mTerba",
      "metadata": {
        "id": "7_XKU1mTerba"
      },
      "outputs": [],
      "source": [
        "X_test_featured = feature_engineer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mCGOvfXoP49v",
      "metadata": {
        "id": "mCGOvfXoP49v"
      },
      "outputs": [],
      "source": [
        "X_test_featured"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3c294e",
      "metadata": {
        "id": "0d3c294e"
      },
      "source": [
        "### Model Definition and Training (XGBoost Regressor)\n",
        "This cell initializes and trains an **XGBoost Regressor**, a powerful gradient-boosted\n",
        "tree-based model well suited for structured/tabular data.\n",
        "The model is trained using the engineered training features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OUAyBdtKU7Re",
      "metadata": {
        "id": "OUAyBdtKU7Re"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "xgb_reg = XGBRegressor(random_state = 42)\n",
        "xgb_reg.fit(X_train_featured,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43829ca",
      "metadata": {
        "id": "b43829ca"
      },
      "source": [
        "### Cross-Validation Score Calculation\n",
        "This cell performs 5-fold cross-validation on the trained XGBoost Regressor model using `neg_root_mean_squared_error` as the scoring metric. This provides a more robust estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "umXwCruTX5B_",
      "metadata": {
        "id": "umXwCruTX5B_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "xgb_cvs = cross_val_score(xgb_reg,X_train_featured,y_train,cv = 5,scoring = 'neg_root_mean_squared_error')\n",
        "xgb_cvs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e262c7ce",
      "metadata": {
        "id": "e262c7ce"
      },
      "source": [
        "### Display Cross-Validation Mean and Standard Deviation\n",
        "This cell calculates and displays the mean and standard deviation of the cross-validation scores, providing a summary of the model's performance across different folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VmYNSB4oYAHn",
      "metadata": {
        "id": "VmYNSB4oYAHn"
      },
      "outputs": [],
      "source": [
        "f'{xgb_cvs.mean()} ± {xgb_cvs.std()}'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cd7f4c",
      "metadata": {
        "id": "50cd7f4c"
      },
      "source": [
        "### Predict on Test Data\n",
        "This cell uses the trained XGBoost Regressor model (`xgb_reg`) to make predictions on the `X_test_featured` dataset. The predictions are stored in `xgb_predicted`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BY9jnu-fYOac",
      "metadata": {
        "id": "BY9jnu-fYOac"
      },
      "outputs": [],
      "source": [
        "xgb_predicted = xgb_reg.predict(X_test_featured)\n",
        "xgb_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e605dd6",
      "metadata": {
        "id": "1e605dd6"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE)\n",
        "This cell calculates the Root Mean Squared Error (RMSE) between the actual target values (`y_test`) and the predicted values (`xgb_predicted`) on the test set. RMSE is a common metric to measure the average magnitude of the errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Kiw6m16YROg",
      "metadata": {
        "id": "5Kiw6m16YROg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import root_mean_squared_error\n",
        "root_mean_squared_error(y_test,xgb_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf6b89f",
      "metadata": {
        "id": "caf6b89f"
      },
      "source": [
        "### Calculate Mean Absolute Error (MAE)\n",
        "This cell calculates the Mean Absolute Error (MAE) between the actual target values (`y_test`) and the predicted values (`xgb_predicted`) on the test set. MAE measures the average magnitude of the errors without considering their direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Th2dIz-CYWFH",
      "metadata": {
        "id": "Th2dIz-CYWFH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test,xgb_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7837b0e",
      "metadata": {
        "id": "b7837b0e"
      },
      "source": [
        "### Calculate R-squared (R2) Score\n",
        "This cell calculates the R-squared (R2) score between the actual target values (`y_test`) and the predicted values (`xgb_predicted`) on the test set. R2 represents the proportion of the variance in the dependent variable that is predictable from the independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Msys0dgCYZJo",
      "metadata": {
        "id": "Msys0dgCYZJo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test,xgb_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314fb588",
      "metadata": {
        "id": "314fb588"
      },
      "source": [
        "### Hyperparameter Tuning with GridSearchCV (XGBoost)\n",
        "This cell performs an exhaustive grid search to find the optimal hyperparameters for the XGBoost Regressor. It defines a parameter grid with various combinations of `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `colsample_bytree`. Cross-validation (`cv=5`) is used to evaluate each combination, with `neg_root_mean_squared_error` as the scoring metric. The best parameters and corresponding RMSE are then printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v6lMzF8TE9S2",
      "metadata": {
        "id": "v6lMzF8TE9S2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [800,1000],\n",
        "    'max_depth': [3,4,5,6],\n",
        "    'learning_rate': [ 0.1,0.2],\n",
        "    'subsample': [ 0.8,0.9],\n",
        "    'colsample_bytree': [0.8,0.9]\n",
        "}\n",
        "\n",
        "xgb_reg = XGBRegressor(random_state=42)\n",
        "\n",
        "grid_search_xgb = GridSearchCV(xgb_reg, param_grid_xgb, cv=5,\n",
        "                               scoring='neg_root_mean_squared_error',\n",
        "                               return_train_score=True, n_jobs=-1, verbose=2)\n",
        "grid_search_xgb.fit(X_train_featured, y_train)\n",
        "\n",
        "print(\"Best parameters for XGBoost: \", grid_search_xgb.best_params_)\n",
        "print(\"Best RMSE for XGBoost: \", -grid_search_xgb.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd957d09",
      "metadata": {
        "id": "cd957d09"
      },
      "source": [
        "### Retrieve Best Estimator\n",
        "This cell retrieves the best performing XGBoost Regressor model found during the `GridSearchCV`. This optimized model (`xgb_best_reg`) will be used for further evaluation and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ik5u1bGZE-DQ",
      "metadata": {
        "id": "ik5u1bGZE-DQ"
      },
      "outputs": [],
      "source": [
        "xgb_best_reg = grid_search_xgb.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RE9HTAiuFf34",
      "metadata": {
        "id": "RE9HTAiuFf34"
      },
      "outputs": [],
      "source": [
        "xgb_best_cvs = cross_val_score(xgb_best_reg,X_train_featured,y_train,cv = 10,scoring = 'neg_root_mean_squared_error')\n",
        "xgb_best_cvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyEUfa5JFnMH",
      "metadata": {
        "id": "vyEUfa5JFnMH"
      },
      "outputs": [],
      "source": [
        "f'{xgb_best_cvs.mean()} ± {xgb_best_cvs.std()}'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87148c01",
      "metadata": {
        "id": "87148c01"
      },
      "source": [
        "### Learning Curve Calculation\n",
        "This cell calculates the learning curve for the XGBoost Regressor. It evaluates the model's performance on both training and cross-validation sets for varying sizes of the training data, helping to diagnose bias and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50aec57",
      "metadata": {
        "id": "e50aec57"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    xgb_best_reg, X_train_featured, y_train,\n",
        "    cv=5, scoring='neg_root_mean_squared_error', train_sizes=train_sizes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5da44c",
      "metadata": {
        "id": "1c5da44c"
      },
      "source": [
        "### Plot Learning Curve\n",
        "This cell plots the learning curve generated in the previous step. It visualizes the training score and cross-validation score against the training set size, along with their standard deviations, to assess model performance and identify potential overfitting or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf870652",
      "metadata": {
        "id": "cf870652"
      },
      "outputs": [],
      "source": [
        "train_scores_mean = -train_scores.mean(axis=1)\n",
        "train_scores_std = train_scores.std(axis=1)\n",
        "test_scores_mean = -test_scores.mean(axis=1)\n",
        "test_scores_std = test_scores.std(axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                 color=\"r\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score\")\n",
        "\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                 color=\"g\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score\")\n",
        "\n",
        "plt.title('Learning Curve (XGBoost)')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0lLWDMtKF1Oz",
      "metadata": {
        "id": "0lLWDMtKF1Oz"
      },
      "outputs": [],
      "source": [
        "xgb_best_predicted = xgb_best_reg.predict(X_test_featured)\n",
        "xgb_best_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHwNf8hjF1CC",
      "metadata": {
        "id": "nHwNf8hjF1CC"
      },
      "outputs": [],
      "source": [
        "root_mean_squared_error(y_test,xgb_best_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iIYy5BUeF0zO",
      "metadata": {
        "id": "iIYy5BUeF0zO"
      },
      "outputs": [],
      "source": [
        "mean_absolute_error(y_test,xgb_best_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dYAOqv2GAnI",
      "metadata": {
        "id": "3dYAOqv2GAnI"
      },
      "outputs": [],
      "source": [
        "r2_score(y_test,xgb_best_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2cb1c6",
      "metadata": {
        "id": "6c2cb1c6"
      },
      "source": [
        "### Calculate Confidence Interval for RMSE\n",
        "This cell calculates a confidence interval for the Root Mean Squared Error (RMSE) of the model's predictions. It first computes the squared differences between the actual (`y_test`) and predicted (`xgb_best_predicted`) target values. Then, using `scipy.stats.t.interval`, it determines a `confidence` (e.g., 95%) interval around the mean of these squared errors. This provides a statistical range, using a t-distribution, to estimate the variability of the model's error.\n",
        "## Note:\n",
        " This confidence interval is an approximate statistical estimate based on\n",
        "the distribution of squared errors and should be interpreted accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Q59ss8ZCDc1",
      "metadata": {
        "id": "3Q59ss8ZCDc1"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "confidence = 0.95\n",
        "squared_errors = (y_test - xgb_best_predicted) ** 2\n",
        "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=np.mean(squared_errors),scale = stats.sem(squared_errors)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe7ef33",
      "metadata": {
        "id": "efe7ef33"
      },
      "source": [
        "### Plot Actual vs. Predicted Values\n",
        "This cell generates a scatter plot comparing the actual target values (`y_test`) against the model's predicted values (`xgb_best_predicted`). A diagonal line representing perfect prediction is also included for reference, helping to visualize model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "irV7iUf1TqM0",
      "metadata": {
        "id": "irV7iUf1TqM0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, xgb_best_predicted, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction') # Added explicit label\n",
        "plt.xlabel('Actual Target Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd3cd74",
      "metadata": {
        "id": "9dd3cd74"
      },
      "source": [
        "### Plot Residuals vs. Predicted Values\n",
        "This cell generates a scatter plot of the residuals (difference between actual and predicted values) against the predicted values. This plot helps to diagnose heteroscedasticity and other patterns in the errors, indicating potential model biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FN7L8Zr8UpxJ",
      "metadata": {
        "id": "FN7L8Zr8UpxJ"
      },
      "outputs": [],
      "source": [
        "residuals = y_test - xgb_best_predicted\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(xgb_best_predicted, residuals, alpha=0.7)\n",
        "plt.hlines(y=0, xmin=xgb_best_predicted.min(), xmax=xgb_best_predicted.max(), colors='red', linestyles='--', label='Perfect Prediction')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals (Actual - Predicted)')\n",
        "plt.title('Residuals vs. Predicted Values')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1151a7a7",
      "metadata": {
        "id": "1151a7a7"
      },
      "outputs": [],
      "source": [
        "feature_importances = xgb_best_reg.feature_importances_\n",
        "features = X_train_featured.columns\n",
        "importances_df = pd.Series(feature_importances, index=features).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f875619",
      "metadata": {
        "id": "7f875619"
      },
      "source": [
        "### Plot Feature Importances\n",
        "This cell visualizes feature importances derived from the **optimized XGBoost Regressor**.\n",
        "These importances are based on the model’s internal split criteria and provide a high-level\n",
        "view of influential features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a5242a",
      "metadata": {
        "id": "f9a5242a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "importances_df.sort_values().plot.barh()\n",
        "plt.title('Feature Importances from Optimised XGBoost Regressor')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d850ab",
      "metadata": {
        "id": "69d850ab"
      },
      "source": [
        "### Calculate Permutation Importances\n",
        "This cell calculates permutation importances for the XGBoost Regressor. Unlike impurity-based feature importances, permutation importance measures the decrease in a model's score when a single feature is randomly shuffled, providing a more reliable estimate of feature contribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a08891",
      "metadata": {
        "id": "44a08891"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "perm_importance = permutation_importance(xgb_best_reg, X_test_featured, y_test, n_repeats=10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c23f9ccc",
      "metadata": {
        "id": "c23f9ccc"
      },
      "source": [
        "### Display Permutation Importances\n",
        "This cell extracts and displays the mean permutation importances, sorted in descending order. This provides a numerical ranking of features based on their impact on model performance when shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a2dc06",
      "metadata": {
        "id": "47a2dc06"
      },
      "outputs": [],
      "source": [
        "perm_importance_mean = perm_importance.importances_mean\n",
        "perm_importances_df = pd.Series(perm_importance_mean, index=X_test_featured.columns).sort_values(ascending=False)\n",
        "perm_importances_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6ef405",
      "metadata": {
        "id": "7f6ef405"
      },
      "source": [
        "### Plot Permutation Importances\n",
        "This cell generates a bar plot of the permutation importances. This visual representation helps to understand the relative importance of each feature in the XGBoost model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7136693b",
      "metadata": {
        "id": "7136693b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "perm_importances_df.plot.bar()\n",
        "plt.title('Permutation Importances from XGBoost Regressor')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance (mean decrease in score)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1fbfd2",
      "metadata": {
        "id": "7b1fbfd2"
      },
      "source": [
        "### Prepare Data for Map Visualization\n",
        "This cell creates a new DataFrame `df_map` containing the `Latitude` and `Longitude` from the test set, along with the actual and predicted median house values. This DataFrame is used for geographical visualization of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb36778",
      "metadata": {
        "id": "4bb36778"
      },
      "outputs": [],
      "source": [
        "df_map = X_test[['latitude', 'longitude']].copy()\n",
        "df_map['Actual_median_house_value'] = y_test\n",
        "df_map['Predicted_median_house_value'] = xgb_best_predicted\n",
        "df_map.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cf5059",
      "metadata": {
        "id": "b6cf5059"
      },
      "source": [
        "### Visualize Actual vs. Predicted Median House Value on Map\n",
        "This cell generates two scatter plots side-by-side, visualizing the geographical distribution of actual and predicted median house values on a map. The size and color of the markers represent the house values, allowing for a visual comparison of model performance across locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef2b255",
      "metadata": {
        "id": "eef2b255"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
        "\n",
        "# Plot for Actual Median House Value\n",
        "df_map.plot(kind='scatter', x='longitude', y='latitude',\n",
        "             c='Actual_median_house_value', cmap='jet', colorbar=True,\n",
        "             s=df_map['Actual_median_house_value']/10000, alpha=0.8, ax=axes[0])\n",
        "axes[0].set_title('Actual Median House Value by Location')\n",
        "axes[0].set_xlabel('Longitude')\n",
        "axes[0].set_ylabel('Latitude')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot for Predicted Median House Value\n",
        "df_map.plot(kind='scatter', x='longitude', y='latitude',\n",
        "             c='Predicted_median_house_value', cmap='jet', colorbar=True,\n",
        "             s=df_map['Predicted_median_house_value']/10000, alpha=0.8, ax=axes[1])\n",
        "axes[1].set_title('Predicted Median House Value by Location')\n",
        "axes[1].set_xlabel('Longitude')\n",
        "axes[1].set_ylabel('Latitude')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5553b2",
      "metadata": {
        "id": "2a5553b2"
      },
      "source": [
        "### Save Trained Model\n",
        "This cell serializes the trained XGBoost regression model using `joblib`.\n",
        "Saving the model enables reuse for inference or deployment without retraining.\n",
        "'California_house_prediction_xgboost_reg.pkl'. This allows for later reuse of the trained model without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Szb8u1Ph2szQ",
      "metadata": {
        "id": "Szb8u1Ph2szQ"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(xgb_best_reg,'California_house_prediction_xgboost_reg.pkl')\n",
        "print(\"XGBoost Regressor model dumped successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EhZ8c383GpoL",
      "metadata": {
        "id": "EhZ8c383GpoL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}